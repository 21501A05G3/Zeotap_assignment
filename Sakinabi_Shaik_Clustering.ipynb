# Import libraries
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import davies_bouldin_score, silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets
customers = pd.read_csv("Customers.csv")
transactions = pd.read_csv("Transactions.csv")

# Step 1: Data Preparation
def prepare_data():
    # Merge customer and transaction data
    combined = pd.merge(transactions, customers, on='CustomerID', how='left')

    # Feature engineering: Total spend, avg spend, purchase frequency, etc.
    customer_features = combined.groupby('CustomerID').agg({
        'TotalValue': ['sum', 'mean'],
        'TransactionID': 'count',
        'Region': 'first'
    }).reset_index()
    customer_features.columns = ['CustomerID', 'TotalSpend', 'AvgSpend', 'PurchaseFrequency', 'Region']

    # Encode Region as one-hot
    customer_features = pd.get_dummies(customer_features, columns=['Region'], drop_first=True)

    return customer_features

# Step 2: Clustering
def perform_clustering(data, n_clusters=5):
    # Scale the data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data.drop(columns=['CustomerID']))

    # Apply K-Means clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(scaled_data)

    # Add cluster labels to the data
    data['Cluster'] = cluster_labels

    # Calculate DB Index
    db_index = davies_bouldin_score(scaled_data, cluster_labels)
    print(f"DB Index for {n_clusters} clusters: {db_index}")

    # Calculate silhouette score
    silhouette_avg = silhouette_score(scaled_data, cluster_labels)
    print(f"Silhouette Score: {silhouette_avg}")

    return data, db_index, silhouette_avg, kmeans

# Step 3: Visualize Clusters
def visualize_clusters(data):
    # Apply PCA for dimensionality reduction
    pca = PCA(n_components=2)
    reduced_data = pca.fit_transform(data.drop(columns=['CustomerID', 'Cluster']))

    # Scatterplot of clusters
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=reduced_data[:, 0], y=reduced_data[:, 1], hue=data['Cluster'], palette='Set2')
    plt.title('Customer Clusters (PCA-reduced)')
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.legend(title='Cluster')
    plt.show()

# Main Function
def main():
    # Prepare data
    customer_features = prepare_data()

    # Perform clustering with optimal clusters (2 to 10 clusters)
    results = []
    for n_clusters in range(2, 11):
        clustered_data, db_index, silhouette_avg, kmeans_model = perform_clustering(customer_features.copy(), n_clusters)
        results.append({'n_clusters': n_clusters, 'db_index': db_index, 'silhouette_avg': silhouette_avg})

    # Find the best number of clusters based on DB Index
    best_result = min(results, key=lambda x: x['db_index'])
    print(f"Optimal Clusters: {best_result['n_clusters']} (DB Index: {best_result['db_index']})")

    # Perform final clustering with optimal clusters
    final_data, _, _, _ = perform_clustering(customer_features.copy(), best_result['n_clusters'])

    # Visualize clusters
    visualize_clusters(final_data)

if __name__ == "__main__":
    main()
